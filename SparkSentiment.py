import osimport pandas as pdfrom pyspark.mllib.feature import HashingTF, IDF, Vectorsfrom pyspark.mllib.classification import NaiveBayesfrom pyspark.mllib.regression import LabeledPointfrom pyspark import SparkContextfrom nltk.corpus import stopwordsfrom random import shuffledatafiles = [{'emo': 'Sad', 'name': "/negative.csv"}, {'emo': 'Happy', 'name': "/positive.csv"}             # {'emo': 'Angry', 'name': "/anger.csv"}, {'emo': "Happy", 'name': "/Happy.csv"}]BASE_DATA_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data'))stopset = set(stopwords.words('english'))class Sentiment(object):    def readFile(self, filepath, header="tweets"):        # bigramData = sc.textFile(contentFile).cache()            return pd.read_csv(BASE_DATA_PATH + filepath, names=[header],                           header=0)    def extractData(self):        htf = HashingTF()        pos_statements = []        neg_statements = []        for value in datafiles:            emo = value['emo']            name = value['name']            read = self.readFile(name)            read['emo'] = emo            if emo == 'Happy':                words = [pos_statements.append(statement.lower().strip()) for statement in read['tweets']]            else:                words = [neg_statements.append(statement.lower().strip()) for statement in read['tweets']]        sparseList = []        for statement in neg_statements:            sparseList.append(LabeledPoint(2.0, htf.transform(word for word in statement.split() if word.lower() not in stopset)))        # sparseList.append(LabeledPoint(0.0, htf.transform(str.split())))        for statement in pos_statements:            sparseList.append(LabeledPoint(1.0, htf.transform(word for word in statement.split() if word.lower() not in stopset)))            # print "statement%s htf %s"%(trs,trs)        model = NaiveBayes.train(sc.parallelize(sparseList),1.0)        #Some example tweets for prediction. When testing take the test input from CSV file                print "Ex1 ", model.predict(htf.transform("I love to view viral content".split()))        print "Ex2", model.predict(htf.transform("This is really bad I am very angry unhappy".split()))        print "Ex3 ", model.predict(htf.transform("I love reading Mashable articles".split()))                # print sparseListsc = SparkContext()sentiment = Sentiment()sentiment.extractData()